{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6c0fa9b",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebd30405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "GPU: NVIDIA A10G\n",
      "Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091aa039",
   "metadata": {},
   "source": [
    "## 2. Configuration and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "207bdaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n",
      "RNN Type: LSTM\n",
      "Hidden Size: 512\n",
      "Num Layers: 3\n",
      "Bidirectional: True\n",
      "Batch size: 32\n",
      "Sequence length: 224\n",
      "Input size per sequence step: 672\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    # Paths\n",
    "    train_dir = './chest_xray_dataset/train'\n",
    "    val_dir = './chest_xray_dataset/val'\n",
    "    test_dir = './chest_xray_dataset/test'\n",
    "    \n",
    "    # Model parameters\n",
    "    num_classes = 3  # Normal, Pneumonia, Tuberculosis\n",
    "    \n",
    "    # Image parameters\n",
    "    img_size = 224\n",
    "    \n",
    "    # RNN-specific parameters\n",
    "    rnn_type = 'LSTM'  # 'LSTM' or 'GRU'\n",
    "    hidden_size = 512\n",
    "    num_layers = 3\n",
    "    bidirectional = True\n",
    "    dropout_rnn = 0.3\n",
    "    \n",
    "    # Sequence parameters (treat image as sequence of rows)\n",
    "    sequence_length = 224  # Number of rows\n",
    "    input_size = 224 * 3  # Each row has 224 pixels × 3 channels\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    batch_size = 32  # Reduced for RNN memory requirements\n",
    "    num_epochs = 50\n",
    "    learning_rate = 0.0005\n",
    "    weight_decay = 1e-4\n",
    "    \n",
    "    # GPU optimization\n",
    "    num_workers = 8\n",
    "    pin_memory = True\n",
    "    prefetch_factor = 4\n",
    "    \n",
    "    # Training settings\n",
    "    early_stopping_patience = 15\n",
    "    lr_scheduler_patience = 5\n",
    "    lr_scheduler_factor = 0.5\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = Config()\n",
    "print(f\"Training on: {config.device}\")\n",
    "print(f\"RNN Type: {config.rnn_type}\")\n",
    "print(f\"Hidden Size: {config.hidden_size}\")\n",
    "print(f\"Num Layers: {config.num_layers}\")\n",
    "print(f\"Bidirectional: {config.bidirectional}\")\n",
    "print(f\"Batch size: {config.batch_size}\")\n",
    "print(f\"Sequence length: {config.sequence_length}\")\n",
    "print(f\"Input size per sequence step: {config.input_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc44981",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "For RNN processing, we'll convert images into sequences. Each image row becomes a sequence step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c8c46f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforms defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((config.img_size, config.img_size)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Validation and test transforms\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((config.img_size, config.img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Transforms defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e459671",
   "metadata": {},
   "source": [
    "## 4. Custom Dataset Wrapper for RNN\n",
    "\n",
    "This wrapper reshapes images into sequences for RNN processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aa6f210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Dataset wrapper defined!\n"
     ]
    }
   ],
   "source": [
    "class RNNImageDataset(Dataset):\n",
    "    \"\"\"Wrapper to convert images into sequences for RNN\"\"\"\n",
    "    def __init__(self, base_dataset):\n",
    "        self.base_dataset = base_dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.base_dataset[idx]\n",
    "        # img shape: (3, 224, 224)\n",
    "        \n",
    "        # Reshape to sequence: (seq_len, input_size)\n",
    "        # Each row of the image becomes a sequence step\n",
    "        # (3, 224, 224) -> (224, 224*3) = (seq_len, features)\n",
    "        img = img.permute(1, 2, 0)  # (224, 224, 3)\n",
    "        img = img.reshape(config.sequence_length, config.input_size)  # (224, 672)\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "print(\"RNN Dataset wrapper defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072b1916",
   "metadata": {},
   "source": [
    "## 5. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bffc945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['normal', 'pneumonia', 'tuberculosis']\n",
      "Train samples: 20450\n",
      "Validation samples: 2534\n",
      "Test samples: 2569\n",
      "Train batches per epoch: 640\n",
      "\n",
      "Sample sequence shape: torch.Size([224, 672])\n",
      "Expected: (224, 672)\n"
     ]
    }
   ],
   "source": [
    "# Load base datasets\n",
    "train_base = ImageFolder(root=config.train_dir, transform=train_transform)\n",
    "val_base = ImageFolder(root=config.val_dir, transform=val_test_transform)\n",
    "test_base = ImageFolder(root=config.test_dir, transform=val_test_transform)\n",
    "\n",
    "# Wrap with RNN dataset\n",
    "train_dataset = RNNImageDataset(train_base)\n",
    "val_dataset = RNNImageDataset(val_base)\n",
    "test_dataset = RNNImageDataset(test_base)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=config.pin_memory,\n",
    "    prefetch_factor=config.prefetch_factor,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=config.pin_memory,\n",
    "    prefetch_factor=config.prefetch_factor,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=config.pin_memory\n",
    ")\n",
    "\n",
    "# Get class names\n",
    "class_names = train_base.classes\n",
    "print(f\"Classes: {class_names}\")\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Train batches per epoch: {len(train_loader)}\")\n",
    "\n",
    "# Test data shape\n",
    "sample_img, sample_label = train_dataset[0]\n",
    "print(f\"\\nSample sequence shape: {sample_img.shape}\")\n",
    "print(f\"Expected: ({config.sequence_length}, {config.input_size})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e83eed",
   "metadata": {},
   "source": [
    "## 6. RNN-based Model Architecture\n",
    "\n",
    "This model processes images as sequences using LSTM/GRU layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b78070c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture: LSTM-based Classifier\n",
      "Total parameters: 18,066,948\n",
      "Trainable parameters: 18,066,948\n",
      "Model moved to: cuda:0\n",
      "\n",
      "Model Summary:\n",
      "ChestXRayRNN(\n",
      "  (input_projection): Sequential(\n",
      "    (0): Linear(in_features=672, out_features=512, bias=True)\n",
      "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (rnn): LSTM(512, 512, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (attention): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.4, inplace=False)\n",
      "    (6): Linear(in_features=256, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ChestXRayRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, \n",
    "                 rnn_type='LSTM', bidirectional=True, dropout=0.3):\n",
    "        super(ChestXRayRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.rnn_type = rnn_type\n",
    "        \n",
    "        # Input projection layer to reduce dimensionality\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # RNN layers\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_size=512,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(\n",
    "                input_size=512,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "        \n",
    "        # Calculate RNN output size\n",
    "        rnn_output_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(rnn_output_size, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_output_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LSTM) or isinstance(m, nn.GRU):\n",
    "                for name, param in m.named_parameters():\n",
    "                    if 'weight_ih' in name:\n",
    "                        nn.init.xavier_uniform_(param)\n",
    "                    elif 'weight_hh' in name:\n",
    "                        nn.init.orthogonal_(param)\n",
    "                    elif 'bias' in name:\n",
    "                        nn.init.constant_(param, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, input_size)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Project input\n",
    "        x = self.input_projection(x)  # (batch, seq_len, 512)\n",
    "        \n",
    "        # RNN forward pass\n",
    "        rnn_out, _ = self.rnn(x)  # (batch, seq_len, hidden_size*2 if bidirectional)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention_weights = self.attention(rnn_out)  # (batch, seq_len, 1)\n",
    "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "        \n",
    "        # Apply attention\n",
    "        context = torch.sum(attention_weights * rnn_out, dim=1)  # (batch, hidden_size*2)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(context)  # (batch, num_classes)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Create model\n",
    "model = ChestXRayRNN(\n",
    "    input_size=config.input_size,\n",
    "    hidden_size=config.hidden_size,\n",
    "    num_layers=config.num_layers,\n",
    "    num_classes=config.num_classes,\n",
    "    rnn_type=config.rnn_type,\n",
    "    bidirectional=config.bidirectional,\n",
    "    dropout=config.dropout_rnn\n",
    ").to(config.device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel Architecture: {config.rnn_type}-based Classifier\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model moved to: {next(model.parameters()).device}\")\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1ea13e",
   "metadata": {},
   "source": [
    "## 7. Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34fa2d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: Adam\n",
      "Initial learning rate: 0.0005\n",
      "Weight decay: 0.0001\n",
      "Mixed precision training: Enabled\n"
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=config.lr_scheduler_factor,\n",
    "    patience=config.lr_scheduler_patience,\n",
    "    # verbose=True\n",
    ")\n",
    "\n",
    "# Mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "print(\"Optimizer: Adam\")\n",
    "print(f\"Initial learning rate: {config.learning_rate}\")\n",
    "print(f\"Weight decay: {config.weight_decay}\")\n",
    "print(\"Mixed precision training: Enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7847222",
   "metadata": {},
   "source": [
    "## 8. Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a83aba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation functions defined!\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, scaler, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc='Training')\n",
    "    \n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Mixed precision forward pass\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Gradient clipping for RNN stability\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100.0 * correct / total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc='Validation')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100.0 * correct / total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(\"Training and validation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714046a9",
   "metadata": {},
   "source": [
    "## 9. Training Loop with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2be743e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 50 epochs...\n",
      "Training on: cuda\n",
      "======================================================================\n",
      "\n",
      "Epoch [1/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:03<00:00, 10.13it/s, loss=0.1218, acc=66.51%]\n",
      "Training: 100%|██████████| 640/640 [01:03<00:00, 10.13it/s, loss=0.1218, acc=66.51%]\n",
      "Validation: 100%|██████████| 80/80 [00:07<00:00, 11.04it/s, loss=0.5860, acc=70.28%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.6654 | Train Acc: 66.51%\n",
      "  Val Loss:   0.5756 | Val Acc:   70.28%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 70.43s\n",
      "  ✓ Best model saved! (Val Loss: 0.5756, Val Acc: 70.28%)\n",
      "\n",
      "Epoch [2/50]\n",
      "----------------------------------------------------------------------\n",
      "  ✓ Best model saved! (Val Loss: 0.5756, Val Acc: 70.28%)\n",
      "\n",
      "Epoch [2/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:01<00:00, 10.49it/s, loss=0.3283, acc=70.39%]\n",
      "Training: 100%|██████████| 640/640 [01:01<00:00, 10.49it/s, loss=0.3283, acc=70.39%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.84it/s, loss=0.6390, acc=70.24%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.5890 | Train Acc: 70.39%\n",
      "  Val Loss:   0.5625 | Val Acc:   70.24%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 67.79s\n",
      "  ✓ Best model saved! (Val Loss: 0.5625, Val Acc: 70.24%)\n",
      "\n",
      "Epoch [3/50]\n",
      "----------------------------------------------------------------------\n",
      "  ✓ Best model saved! (Val Loss: 0.5625, Val Acc: 70.24%)\n",
      "\n",
      "Epoch [3/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.55it/s, loss=0.7798, acc=71.73%]\n",
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.55it/s, loss=0.7798, acc=71.73%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.87it/s, loss=0.5452, acc=71.19%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.5629 | Train Acc: 71.73%\n",
      "  Val Loss:   0.5549 | Val Acc:   71.19%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 67.39s\n",
      "  ✓ Best model saved! (Val Loss: 0.5549, Val Acc: 71.19%)\n",
      "\n",
      "Epoch [4/50]\n",
      "----------------------------------------------------------------------\n",
      "  ✓ Best model saved! (Val Loss: 0.5549, Val Acc: 71.19%)\n",
      "\n",
      "Epoch [4/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.61it/s, loss=0.4278, acc=72.44%]\n",
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.61it/s, loss=0.4278, acc=72.44%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.88it/s, loss=0.6458, acc=71.35%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.5371 | Train Acc: 72.44%\n",
      "  Val Loss:   0.5350 | Val Acc:   71.35%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 67.06s\n",
      "  ✓ Best model saved! (Val Loss: 0.5350, Val Acc: 71.35%)\n",
      "\n",
      "Epoch [5/50]\n",
      "----------------------------------------------------------------------\n",
      "  ✓ Best model saved! (Val Loss: 0.5350, Val Acc: 71.35%)\n",
      "\n",
      "Epoch [5/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.62it/s, loss=0.7947, acc=72.50%]\n",
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.62it/s, loss=0.7947, acc=72.50%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.88it/s, loss=0.4463, acc=73.28%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.5329 | Train Acc: 72.50%\n",
      "  Val Loss:   0.5209 | Val Acc:   73.28%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 67.01s\n",
      "  ✓ Best model saved! (Val Loss: 0.5209, Val Acc: 73.28%)\n",
      "\n",
      "Epoch [6/50]\n",
      "----------------------------------------------------------------------\n",
      "  ✓ Best model saved! (Val Loss: 0.5209, Val Acc: 73.28%)\n",
      "\n",
      "Epoch [6/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:01<00:00, 10.37it/s, loss=0.3412, acc=73.10%]\n",
      "Training: 100%|██████████| 640/640 [01:01<00:00, 10.37it/s, loss=0.3412, acc=73.10%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.91it/s, loss=0.4975, acc=72.65%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.91it/s, loss=0.4975, acc=72.65%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.5203 | Train Acc: 73.10%\n",
      "  Val Loss:   0.5104 | Val Acc:   72.65%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 68.46s\n",
      "  ✓ Best model saved! (Val Loss: 0.5104, Val Acc: 72.65%)\n",
      "\n",
      "Epoch [7/50]\n",
      "----------------------------------------------------------------------\n",
      "  ✓ Best model saved! (Val Loss: 0.5104, Val Acc: 72.65%)\n",
      "\n",
      "Epoch [7/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.58it/s, loss=0.4388, acc=73.43%]\n",
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.58it/s, loss=0.4388, acc=73.43%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 12.01it/s, loss=0.3835, acc=72.65%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 12.01it/s, loss=0.3835, acc=72.65%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.5143 | Train Acc: 73.43%\n",
      "  Val Loss:   0.5279 | Val Acc:   72.65%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 67.17s\n",
      "  Early stopping: 1/15\n",
      "\n",
      "Epoch [8/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.53it/s, loss=0.1533, acc=73.76%]\n",
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.53it/s, loss=0.1533, acc=73.76%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.72it/s, loss=0.6674, acc=72.65%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.5072 | Train Acc: 73.76%\n",
      "  Val Loss:   0.4928 | Val Acc:   72.65%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 67.62s\n",
      "  ✓ Best model saved! (Val Loss: 0.4928, Val Acc: 72.65%)\n",
      "\n",
      "Epoch [9/50]\n",
      "----------------------------------------------------------------------\n",
      "  ✓ Best model saved! (Val Loss: 0.4928, Val Acc: 72.65%)\n",
      "\n",
      "Epoch [9/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.60it/s, loss=0.7673, acc=73.69%]\n",
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.60it/s, loss=0.7673, acc=73.69%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.82it/s, loss=0.6012, acc=73.48%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.82it/s, loss=0.6012, acc=73.48%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.5033 | Train Acc: 73.69%\n",
      "  Val Loss:   0.4949 | Val Acc:   73.48%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 67.15s\n",
      "  Early stopping: 1/15\n",
      "\n",
      "Epoch [10/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.60it/s, loss=0.7375, acc=73.90%]\n",
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.60it/s, loss=0.7375, acc=73.90%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.99it/s, loss=0.7788, acc=72.26%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.99it/s, loss=0.7788, acc=72.26%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.5006 | Train Acc: 73.90%\n",
      "  Val Loss:   0.4989 | Val Acc:   72.26%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 67.07s\n",
      "  Early stopping: 2/15\n",
      "\n",
      "Epoch [11/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.55it/s, loss=0.9197, acc=74.11%]\n",
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.55it/s, loss=0.9197, acc=74.11%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.88it/s, loss=0.5845, acc=74.03%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.4955 | Train Acc: 74.11%\n",
      "  Val Loss:   0.4880 | Val Acc:   74.03%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 67.41s\n",
      "  ✓ Best model saved! (Val Loss: 0.4880, Val Acc: 74.03%)\n",
      "\n",
      "Epoch [12/50]\n",
      "----------------------------------------------------------------------\n",
      "  ✓ Best model saved! (Val Loss: 0.4880, Val Acc: 74.03%)\n",
      "\n",
      "Epoch [12/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.57it/s, loss=0.4777, acc=74.32%]\n",
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.57it/s, loss=0.4777, acc=74.32%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.83it/s, loss=0.7669, acc=72.89%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.83it/s, loss=0.7669, acc=72.89%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.4917 | Train Acc: 74.32%\n",
      "  Val Loss:   0.4966 | Val Acc:   72.89%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 67.33s\n",
      "  Early stopping: 1/15\n",
      "\n",
      "Epoch [13/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.55it/s, loss=0.3922, acc=73.92%]\n",
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.55it/s, loss=0.3922, acc=73.92%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.89it/s, loss=0.5484, acc=73.80%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.89it/s, loss=0.5484, acc=73.80%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.4917 | Train Acc: 73.92%\n",
      "  Val Loss:   0.4943 | Val Acc:   73.80%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 67.40s\n",
      "  Early stopping: 2/15\n",
      "\n",
      "Epoch [14/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.55it/s, loss=0.2372, acc=74.51%]\n",
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.55it/s, loss=0.2372, acc=74.51%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.94it/s, loss=0.6002, acc=73.16%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.94it/s, loss=0.6002, acc=73.16%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.4873 | Train Acc: 74.51%\n",
      "  Val Loss:   0.4976 | Val Acc:   73.16%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 67.37s\n",
      "  Early stopping: 3/15\n",
      "\n",
      "Epoch [15/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.56it/s, loss=0.3843, acc=74.39%]\n",
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.56it/s, loss=0.3843, acc=74.39%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.84it/s, loss=0.5342, acc=73.16%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.84it/s, loss=0.5342, acc=73.16%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.4884 | Train Acc: 74.39%\n",
      "  Val Loss:   0.5052 | Val Acc:   73.16%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 67.36s\n",
      "  Early stopping: 4/15\n",
      "\n",
      "Epoch [16/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.64it/s, loss=0.0001, acc=74.19%]\n",
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.64it/s, loss=0.0001, acc=74.19%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.79it/s, loss=0.6195, acc=73.20%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.79it/s, loss=0.6195, acc=73.20%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.4812 | Train Acc: 74.19%\n",
      "  Val Loss:   0.4936 | Val Acc:   73.20%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 66.92s\n",
      "  Early stopping: 5/15\n",
      "\n",
      "Epoch [17/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.62it/s, loss=0.8899, acc=74.30%]\n",
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.62it/s, loss=0.8899, acc=74.30%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.82it/s, loss=0.6249, acc=73.60%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.4829 | Train Acc: 74.30%\n",
      "  Val Loss:   0.4869 | Val Acc:   73.60%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 67.06s\n",
      "  ✓ Best model saved! (Val Loss: 0.4869, Val Acc: 73.60%)\n",
      "\n",
      "Epoch [18/50]\n",
      "----------------------------------------------------------------------\n",
      "  ✓ Best model saved! (Val Loss: 0.4869, Val Acc: 73.60%)\n",
      "\n",
      "Epoch [18/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.62it/s, loss=0.2645, acc=74.29%]\n",
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.62it/s, loss=0.2645, acc=74.29%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 12.02it/s, loss=0.5655, acc=73.68%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 12.02it/s, loss=0.5655, acc=73.68%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.4809 | Train Acc: 74.29%\n",
      "  Val Loss:   0.4932 | Val Acc:   73.68%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 66.90s\n",
      "  Early stopping: 1/15\n",
      "\n",
      "Epoch [19/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.61it/s, loss=0.5552, acc=74.14%]\n",
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.61it/s, loss=0.5552, acc=74.14%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 12.00it/s, loss=0.5943, acc=73.88%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.4831 | Train Acc: 74.14%\n",
      "  Val Loss:   0.4762 | Val Acc:   73.88%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 67.01s\n",
      "  ✓ Best model saved! (Val Loss: 0.4762, Val Acc: 73.88%)\n",
      "\n",
      "Epoch [20/50]\n",
      "----------------------------------------------------------------------\n",
      "  ✓ Best model saved! (Val Loss: 0.4762, Val Acc: 73.88%)\n",
      "\n",
      "Epoch [20/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.55it/s, loss=0.8313, acc=74.59%]\n",
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.55it/s, loss=0.8313, acc=74.59%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.86it/s, loss=0.6052, acc=73.60%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.86it/s, loss=0.6052, acc=73.60%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.4795 | Train Acc: 74.59%\n",
      "  Val Loss:   0.4852 | Val Acc:   73.60%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 67.39s\n",
      "  Early stopping: 1/15\n",
      "\n",
      "Epoch [21/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.57it/s, loss=0.0920, acc=74.60%]\n",
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.57it/s, loss=0.0920, acc=74.60%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 12.00it/s, loss=0.5497, acc=73.95%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.4783 | Train Acc: 74.60%\n",
      "  Val Loss:   0.4729 | Val Acc:   73.95%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 67.21s\n",
      "  ✓ Best model saved! (Val Loss: 0.4729, Val Acc: 73.95%)\n",
      "\n",
      "Epoch [22/50]\n",
      "----------------------------------------------------------------------\n",
      "  ✓ Best model saved! (Val Loss: 0.4729, Val Acc: 73.95%)\n",
      "\n",
      "Epoch [22/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.59it/s, loss=0.0172, acc=74.32%]\n",
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.59it/s, loss=0.0172, acc=74.32%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.97it/s, loss=0.5695, acc=73.64%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.97it/s, loss=0.5695, acc=73.64%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.4740 | Train Acc: 74.32%\n",
      "  Val Loss:   0.4803 | Val Acc:   73.64%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 67.13s\n",
      "  Early stopping: 1/15\n",
      "\n",
      "Epoch [23/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.57it/s, loss=0.0005, acc=74.86%]\n",
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.57it/s, loss=0.0005, acc=74.86%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.96it/s, loss=0.4542, acc=74.11%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.96it/s, loss=0.4542, acc=74.11%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.4770 | Train Acc: 74.86%\n",
      "  Val Loss:   0.4851 | Val Acc:   74.11%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 67.26s\n",
      "  Early stopping: 2/15\n",
      "\n",
      "Epoch [24/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.57it/s, loss=0.4653, acc=74.67%]\n",
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.57it/s, loss=0.4653, acc=74.67%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.76it/s, loss=0.4894, acc=74.19%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.76it/s, loss=0.4894, acc=74.19%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.4732 | Train Acc: 74.67%\n",
      "  Val Loss:   0.5290 | Val Acc:   74.19%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 67.33s\n",
      "  Early stopping: 3/15\n",
      "\n",
      "Epoch [25/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.59it/s, loss=0.4584, acc=74.54%]\n",
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.59it/s, loss=0.4584, acc=74.54%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.91it/s, loss=0.5354, acc=73.99%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.4754 | Train Acc: 74.54%\n",
      "  Val Loss:   0.4679 | Val Acc:   73.99%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 67.14s\n",
      "  ✓ Best model saved! (Val Loss: 0.4679, Val Acc: 73.99%)\n",
      "\n",
      "Epoch [26/50]\n",
      "----------------------------------------------------------------------\n",
      "  ✓ Best model saved! (Val Loss: 0.4679, Val Acc: 73.99%)\n",
      "\n",
      "Epoch [26/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.65it/s, loss=0.1867, acc=74.61%]\n",
      "Training: 100%|██████████| 640/640 [01:00<00:00, 10.65it/s, loss=0.1867, acc=74.61%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.84it/s, loss=0.5627, acc=74.15%]\n",
      "Validation: 100%|██████████| 80/80 [00:06<00:00, 11.84it/s, loss=0.5627, acc=74.15%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "  Train Loss: 0.4743 | Train Acc: 74.61%\n",
      "  Val Loss:   0.4764 | Val Acc:   74.15%\n",
      "  Learning Rate: 0.000500\n",
      "  Epoch Time: 66.87s\n",
      "  Early stopping: 1/15\n",
      "\n",
      "Epoch [27/50]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/640 [00:01<12:56,  1.21s/it, loss=0.4005, acc=81.25%]"
     ]
    }
   ],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "# Early stopping variables\n",
    "best_val_loss = float('inf')\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "best_model_path = 'best_rnn_model.pth'\n",
    "\n",
    "print(f\"Starting training for {config.num_epochs} epochs...\")\n",
    "print(f\"Training on: {config.device}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    print(f\"\\nEpoch [{epoch+1}/{config.num_epochs}]\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, scaler, config.device\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate_epoch(\n",
    "        model, val_loader, criterion, config.device\n",
    "    )\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "    print(f\"  Learning Rate: {current_lr:.6f}\")\n",
    "    print(f\"  Epoch Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "        }, best_model_path)\n",
    "        print(f\"  ✓ Best model saved! (Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%)\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  Early stopping: {patience_counter}/{config.early_stopping_patience}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= config.early_stopping_patience:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch+1} epochs!\")\n",
    "        break\n",
    "    \n",
    "    # Clear cache periodically\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Training completed in {total_time/60:.2f} minutes\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeacf77",
   "metadata": {},
   "source": [
    "## 10. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f006cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('RNN Model: Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train Acc', marker='o')\n",
    "axes[1].plot(history['val_acc'], label='Val Acc', marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('RNN Model: Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot learning rate\n",
    "axes[2].plot(history['lr'], marker='o', color='green')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Learning Rate')\n",
    "axes[2].set_title('RNN Model: Learning Rate Schedule')\n",
    "axes[2].set_yscale('log')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rnn_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training history plots saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7393f80e",
   "metadata": {},
   "source": [
    "## 11. Load Best Model and Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd5b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(best_model_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Best model loaded from epoch {checkpoint['epoch']+1}\")\n",
    "print(f\"Best validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "print(f\"Best validation accuracy: {checkpoint['val_acc']:.2f}%\")\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "test_loss = 0.0\n",
    "\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader):\n",
    "        inputs = inputs.to(config.device, non_blocking=True)\n",
    "        labels = labels.to(config.device, non_blocking=True)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_loss = test_loss / len(test_dataset)\n",
    "test_acc = 100.0 * np.sum(np.array(all_preds) == np.array(all_labels)) / len(all_labels)\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d4e17b",
   "metadata": {},
   "source": [
    "## 12. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d5bf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "report = classification_report(all_labels, all_preds, target_names=class_names, digits=4)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\"*70)\n",
    "print(report)\n",
    "\n",
    "# Save report to file\n",
    "with open('rnn_classification_report.txt', 'w') as f:\n",
    "    f.write(\"RNN Model Classification Report\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    f.write(f\"Model Type: {config.rnn_type}\\n\")\n",
    "    f.write(f\"Hidden Size: {config.hidden_size}\\n\")\n",
    "    f.write(f\"Num Layers: {config.num_layers}\\n\")\n",
    "    f.write(f\"Bidirectional: {config.bidirectional}\\n\\n\")\n",
    "    f.write(report)\n",
    "print(\"Classification report saved to 'rnn_classification_report.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9947c84e",
   "metadata": {},
   "source": [
    "## 13. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12350782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and plot confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.title('RNN Model - Confusion Matrix (Test Set)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('rnn_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "print(\"\\nPer-class accuracy:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_acc = 100.0 * cm[i, i] / cm[i].sum()\n",
    "    print(f\"  {class_name}: {class_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0228c5a",
   "metadata": {},
   "source": [
    "## 14. Visualize Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8d7c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some test samples for visualization\n",
    "# We need to get original images, not sequences\n",
    "test_base_loader = DataLoader(\n",
    "    test_base,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "def denormalize(tensor):\n",
    "    \"\"\"Denormalize image for visualization\"\"\"\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    return tensor * std + mean\n",
    "\n",
    "# Get a batch\n",
    "dataiter = iter(test_base_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Convert to sequences for prediction\n",
    "sequences = []\n",
    "for img in images:\n",
    "    img_seq = img.permute(1, 2, 0).reshape(config.sequence_length, config.input_size)\n",
    "    sequences.append(img_seq)\n",
    "sequences = torch.stack(sequences).to(config.device)\n",
    "\n",
    "# Get predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        outputs = model(sequences)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Plot sample predictions\n",
    "fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx in range(16):\n",
    "    if idx < len(images):\n",
    "        img = denormalize(images[idx])\n",
    "        img = img.permute(1, 2, 0).numpy()\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        true_label = class_names[labels[idx]]\n",
    "        pred_label = class_names[predicted[idx].cpu()]\n",
    "        \n",
    "        color = 'green' if labels[idx] == predicted[idx].cpu() else 'red'\n",
    "        \n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].axis('off')\n",
    "        axes[idx].set_title(f'True: {true_label}\\nPred: {pred_label}', \n",
    "                           color=color, fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.suptitle('RNN Model - Sample Predictions', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('rnn_sample_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Sample predictions visualized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b60c2f",
   "metadata": {},
   "source": [
    "## 15. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d09cac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'class_names': class_names,\n",
    "    'config': {\n",
    "        'num_classes': config.num_classes,\n",
    "        'img_size': config.img_size,\n",
    "        'rnn_type': config.rnn_type,\n",
    "        'hidden_size': config.hidden_size,\n",
    "        'num_layers': config.num_layers,\n",
    "        'bidirectional': config.bidirectional,\n",
    "        'input_size': config.input_size,\n",
    "        'sequence_length': config.sequence_length,\n",
    "    },\n",
    "    'test_accuracy': test_acc,\n",
    "    'history': history\n",
    "}, 'chest_xray_rnn_final.pth')\n",
    "\n",
    "print(\"Final model saved as 'chest_xray_rnn_final.pth'\")\n",
    "print(\"\\nModel Summary:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Test accuracy: {test_acc:.2f}%\")\n",
    "print(f\"  Classes: {class_names}\")\n",
    "print(f\"  RNN Type: {config.rnn_type}\")\n",
    "print(f\"  Hidden Size: {config.hidden_size}\")\n",
    "print(f\"  Num Layers: {config.num_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cef342e",
   "metadata": {},
   "source": [
    "## 16. Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1982e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# CNN results (from previous training)\n",
    "cnn_results = {\n",
    "    'Model': 'Custom CNN',\n",
    "    'Test Accuracy': '75.24%',\n",
    "    'Normal Recall': '85.73%',\n",
    "    'Pneumonia Recall': '92.07%',\n",
    "    'Tuberculosis Recall': '56.95%'\n",
    "}\n",
    "\n",
    "# RNN results (current model)\n",
    "rnn_results = {\n",
    "    'Model': f'{config.rnn_type}-based',\n",
    "    'Test Accuracy': f'{test_acc:.2f}%',\n",
    "}\n",
    "\n",
    "# Calculate per-class recall for RNN\n",
    "from sklearn.metrics import recall_score\n",
    "recalls = recall_score(all_labels, all_preds, average=None)\n",
    "for i, class_name in enumerate(class_names):\n",
    "    rnn_results[f'{class_name} Recall'] = f'{recalls[i]*100:.2f}%'\n",
    "\n",
    "print(\"\\nCNN Model (Previous):\")\n",
    "for key, value in cnn_results.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\n{config.rnn_type} Model (Current):\")\n",
    "for key, value in rnn_results.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHTS:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n1. Architecture Comparison:\")\n",
    "print(\"   - CNN: Spatial feature extraction, ~40M parameters\")\n",
    "print(f\"   - RNN: Sequential processing with attention, ~{total_params/1e6:.1f}M parameters\")\n",
    "print(\"\\n2. Processing Approach:\")\n",
    "print(\"   - CNN: Processes images as 2D spatial data\")\n",
    "print(\"   - RNN: Treats images as sequences of rows with temporal dependencies\")\n",
    "print(\"\\n3. Computational Efficiency:\")\n",
    "print(\"   - CNN: Parallel convolution operations\")\n",
    "print(\"   - RNN: Sequential processing (slower but captures dependencies)\")\n",
    "print(\"\\n4. Best Use Cases:\")\n",
    "print(\"   - CNN: Standard choice for image classification (spatial features)\")\n",
    "print(\"   - RNN: Experimental approach for sequential pattern analysis\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1bb85b",
   "metadata": {},
   "source": [
    "## 17. GPU Utilization Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f6abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Memory Summary:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"Reserved:  {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"Max Allocated: {torch.cuda.max_memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU cache cleared!\")\n",
    "\n",
    "print(\"\\n🎉 RNN Training pipeline completed successfully!\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  - best_rnn_model.pth (best model checkpoint)\")\n",
    "print(\"  - chest_xray_rnn_final.pth (final model with metadata)\")\n",
    "print(\"  - rnn_training_history.png (training curves)\")\n",
    "print(\"  - rnn_confusion_matrix.png (confusion matrix)\")\n",
    "print(\"  - rnn_sample_predictions.png (sample predictions)\")\n",
    "print(\"  - rnn_classification_report.txt (detailed metrics)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9fa0be",
   "metadata": {},
   "source": [
    "## 18. Final Recommendations\n",
    "\n",
    "### Model Selection Guidelines:\n",
    "\n",
    "**Choose CNN if:**\n",
    "- You need standard, proven image classification performance\n",
    "- Spatial features (edges, textures, patterns) are most important\n",
    "- Faster training and inference are priorities\n",
    "- You want simpler model architecture\n",
    "\n",
    "**Choose RNN if:**\n",
    "- You want to explore sequential dependencies in image data\n",
    "- You're interested in experimental approaches\n",
    "- You have sufficient computational resources\n",
    "- You want attention mechanisms to identify important regions\n",
    "\n",
    "### Hybrid Approach:\n",
    "Consider combining both:\n",
    "- Use CNN for feature extraction\n",
    "- Feed CNN features into RNN for sequential modeling\n",
    "- This is the basis for advanced architectures like CNN-LSTM\n",
    "\n",
    "### Next Steps:\n",
    "1. Try transfer learning with pre-trained models (ResNet, EfficientNet)\n",
    "2. Implement ensemble methods combining multiple models\n",
    "3. Use attention mechanisms (Vision Transformers)\n",
    "4. Address class imbalance with weighted loss or sampling strategies\n",
    "5. Collect more data, especially for Tuberculosis class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
