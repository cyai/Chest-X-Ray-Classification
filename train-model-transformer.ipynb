{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca6d830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa6856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    # Paths\n",
    "    data_dir = '/home/ubuntu/dl/chest_xray_dataset'\n",
    "    train_dir = os.path.join(data_dir, 'train')\n",
    "    val_dir = os.path.join(data_dir, 'val')\n",
    "    test_dir = os.path.join(data_dir, 'test')\n",
    "    \n",
    "    # Model parameters\n",
    "    img_size = 224\n",
    "    patch_size = 16\n",
    "    num_patches = (img_size // patch_size) ** 2  # 196\n",
    "    embed_dim = 512\n",
    "    num_heads = 8\n",
    "    num_layers = 6\n",
    "    mlp_ratio = 4\n",
    "    dropout = 0.1\n",
    "    num_classes = 3\n",
    "    \n",
    "    # Training parameters\n",
    "    batch_size = 32\n",
    "    num_epochs = 50\n",
    "    learning_rate = 1e-4\n",
    "    weight_decay = 1e-4\n",
    "    patience = 7\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "config = Config()\n",
    "print(f\"Image size: {config.img_size}x{config.img_size}\")\n",
    "print(f\"Patch size: {config.patch_size}x{config.patch_size}\")\n",
    "print(f\"Number of patches: {config.num_patches}\")\n",
    "print(f\"Embedding dimension: {config.embed_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae97d07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class ChestXRayDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = ['normal', 'pneumonia', 'tuberculosis']\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.samples = self._load_samples()\n",
    "    \n",
    "    def _load_samples(self):\n",
    "        samples = []\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(self.root_dir, class_name)\n",
    "            if not os.path.exists(class_dir):\n",
    "                continue\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    img_path = os.path.join(class_dir, img_name)\n",
    "                    samples.append((img_path, self.class_to_idx[class_name]))\n",
    "        return samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb199fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((config.img_size, config.img_size)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((config.img_size, config.img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ChestXRayDataset(config.train_dir, transform=train_transform)\n",
    "val_dataset = ChestXRayDataset(config.val_dir, transform=val_transform)\n",
    "test_dataset = ChestXRayDataset(config.test_dir, transform=val_transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch Embedding Layer\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=512):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        x = self.proj(x)  # (B, embed_dim, H/P, W/P)\n",
    "        x = x.flatten(2)  # (B, embed_dim, num_patches)\n",
    "        x = x.transpose(1, 2)  # (B, num_patches, embed_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24739ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Self-Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Generate Q, K, V\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29c632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Block\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, embed_dim, mlp_ratio=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707adf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Encoder Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, mlp_ratio, dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f808e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision Transformer\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, num_classes=3,\n",
    "                 embed_dim=512, num_heads=8, num_layers=6, mlp_ratio=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        \n",
    "        # Class token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        # Positional embedding\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)  # (B, num_patches, embed_dim)\n",
    "        \n",
    "        # Add class token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # (B, num_patches+1, embed_dim)\n",
    "        \n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Classification\n",
    "        x = self.norm(x)\n",
    "        cls_token_final = x[:, 0]  # Take class token\n",
    "        x = self.head(cls_token_final)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dba2aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = VisionTransformer(\n",
    "    img_size=config.img_size,\n",
    "    patch_size=config.patch_size,\n",
    "    in_channels=3,\n",
    "    num_classes=config.num_classes,\n",
    "    embed_dim=config.embed_dim,\n",
    "    num_heads=config.num_heads,\n",
    "    num_layers=config.num_layers,\n",
    "    mlp_ratio=config.mlp_ratio,\n",
    "    dropout=config.dropout\n",
    ").to(config.device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ad19db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.num_epochs)\n",
    "\n",
    "print(\"Training setup complete\")\n",
    "print(f\"Loss function: CrossEntropyLoss\")\n",
    "print(f\"Optimizer: AdamW (lr={config.learning_rate}, weight_decay={config.weight_decay})\")\n",
    "print(f\"Scheduler: CosineAnnealingLR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ba199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100*correct/total:.2f}%'})\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0904f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation function\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc='Validation'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da1611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with early stopping\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_acc': []\n",
    "}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "print(f\"Starting training for {config.num_epochs} epochs...\")\n",
    "print(f\"Early stopping patience: {config.patience} epochs\\n\")\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    print(f\"Epoch [{epoch+1}/{config.num_epochs}]\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, config.device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, config.device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    print(f\"Learning Rate: {scheduler.get_last_lr()[0]:.6f}\\n\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_vit_model.pth')\n",
    "        print(f\"✓ New best model saved (Val Acc: {val_acc:.2f}%)\\n\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= config.patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268dabe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "ax1.plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(history['train_acc'], label='Train Acc', marker='o')\n",
    "ax2.plot(history['val_acc'], label='Val Acc', marker='s')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('vit_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training history plot saved as 'vit_training_history.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17212ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for evaluation\n",
    "model.load_state_dict(torch.load('best_vit_model.pth'))\n",
    "print(\"Loaded best model for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e477af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluation\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc='Testing'):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    return np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "# Get predictions\n",
    "y_true, y_pred = evaluate_model(model, test_loader, config.device)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_acc = 100 * np.sum(y_true == y_pred) / len(y_true)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ebefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "class_names = ['Normal', 'Pneumonia', 'Tuberculosis']\n",
    "report = classification_report(y_true, y_pred, target_names=class_names, digits=4)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Save report\n",
    "with open('vit_classification_report.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "print(\"Classification report saved to 'vit_classification_report.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cccf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Vision Transformer', fontsize=16, pad=20)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('vit_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrix saved as 'vit_confusion_matrix.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b33e18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model with metadata\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': {\n",
    "        'img_size': config.img_size,\n",
    "        'patch_size': config.patch_size,\n",
    "        'embed_dim': config.embed_dim,\n",
    "        'num_heads': config.num_heads,\n",
    "        'num_layers': config.num_layers,\n",
    "        'num_classes': config.num_classes,\n",
    "    },\n",
    "    'test_accuracy': test_acc,\n",
    "    'best_val_accuracy': best_val_acc,\n",
    "    'class_names': class_names\n",
    "}, 'chest_xray_vit_final.pth')\n",
    "\n",
    "print(\"Final model saved as 'chest_xray_vit_final.pth'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfa81a5",
   "metadata": {},
   "source": [
    "## Model Summary\n",
    "\n",
    "**Vision Transformer Architecture:**\n",
    "- **Patch Size**: 16×16 pixels (196 patches per image)\n",
    "- **Embedding Dimension**: 512\n",
    "- **Transformer Layers**: 6\n",
    "- **Attention Heads**: 8 per layer\n",
    "- **MLP Ratio**: 4 (hidden dim = 2048)\n",
    "- **Dropout**: 0.1\n",
    "\n",
    "**Key Components:**\n",
    "1. **Patch Embedding**: Converts image into sequence of patches\n",
    "2. **CLS Token**: Learnable classification token\n",
    "3. **Positional Encoding**: Adds spatial information\n",
    "4. **Multi-Head Self-Attention**: Captures global dependencies\n",
    "5. **Feed-Forward Networks**: Per-token processing\n",
    "6. **Layer Normalization**: Stabilizes training\n",
    "\n",
    "**Training Configuration:**\n",
    "- Optimizer: AdamW (lr=1e-4, weight_decay=1e-4)\n",
    "- Scheduler: CosineAnnealingLR\n",
    "- Batch Size: 32\n",
    "- Early Stopping: Patience 7 epochs\n",
    "- Data Augmentation: Random flips, rotation, color jitter"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
